# ベイスの定理
https://ai-trend.jp/basic-study/bayes/bayes-theorem/

# 勾配ブースティング
テーブルデータの教師あり学習において幅広いデータセットで高い精度を出すモデルとして知られている。弱い学習器を逐次的に学習するモデルである。
弱学習器としては具体的なモデルを仮定していない。
勾配降下法を使う

## 勾配ブースティング木（GBDT）
弱学習器に決定木を使用したもの
xgboost・lightGBMとかこれ

# ランダムフォレスト
バギングと呼ばれるサンプルの重複ありランダムサンプリングと各決定木で使用する特徴量のランダムサンプリングを行っている。

# ロジステック回帰
シグモイドが出力層なので0.5よりデカいか否か

# 決定木における特徴量重要度
## 分割回数(frequency)
それぞれの特徴量の分割回数
## 不純度

# 正規化
最小値を0，最大値を1とする0-1スケーリング手法
異常に小さかったり、大きかったりする外れ値がある場合はデータが偏ってしまうので注意が必要
# 標準化
平均を0，分散を1とするスケーリング手法
最大値や最小値が決まっていないため特定のアルゴリズムで問題になったりする
一般的には標準化の方がよく使う

# バイアスとバリアンス
訓練をすればするほどバイアスは低くなるが、一方でバリアンスは高くなりがち（トレードオフの関係）
## バイアス
簡単にいうと実際値と予測値の差
## バリアンス
予測値の散らばってる度合い

# アンサンブル学習
## バギング
それぞれ学習した多数の弱学習器の多数決（分類の場合　回帰の場合は平均値）を取ることによって汎化性能を高めたアンサンブル学習のこと
並列的
## ブートストラップサンプリング（重複ありランダムサンプリング）
重複を許して無作為同数リサンプリングを反復する方法
ちなみに重複を許さないのがジャックナイフ法
## ブースティング
直列的に弱学習器を用いる。学習器を連続的に学習させて、より精度が向上するように修正していく
バイアスは下がりやすく、バリアンスが高くなりやすい、過学習に注意
## スタッキング
第一段階として学習器にランダムフォレストや勾配ブースティングなどのさまざまな計算法を使って複数のモデルを用意し学習し、予測値を出力（データセットは分けてあるモデルで学習に使ったデータセットと予測に使うデータセットはそれぞれ別にしないと過学習になる）
第一段階の予測値を取りまとめるモデルをメタモデルという。メタモデルは第一段階の予測値を特徴量として学習して、最終の予測値を出す。
メタモデルには回帰ならよく線形回帰モデルが使われるが特にアルゴリズムの決まりはない
[アンサンブル学習の種類](https://toukei-lab.com/ensemble#i-2)

# バッチ正規化
- 各層に伝わってきたデータを、その層でまた正規化するアプローチ。（最初に正規化をするだけでなく、層ごとに正規化を繰り返す）
- データの正規化、重みの初期化と比較し、より直接的な手法となる。
- 非常に強力な手法で学習がうまくいきやすく、オーバーフィッティングしにくい。
## 内部共変量シフト
入力時のxの分布が、学習途中で大きく変わってくることを指す。このシフトに合わせることにweightたちは躍起になるので、層自体の学習はそのあとでしか進まない。これのせいで遅くなる。
バッチ正規化は、このシフトを正規化してくれるもの。

# 転移学習
学習済みモデルの中間層をそのまま利用して最終出力層のみ付け替えることで他の問題に適用する
ここで重要なのは`中間層の重みづけはそのまま利用`すること

メリット
1. 既に学習済であるため新規の学習データの量はデータは少なくてよい
2. 既に学習済であるため新規の学習データによる計算量は少ない

# ファインチューニング
学習済みモデルの最終出力層を付け替えるのですが、入力層に近い部分と出力層のパラメータも変更します。

# 蒸留
学習済みのモデルを使っており`コンパクト`なモデルを実現することです。転移学習との大きな違いはコンパクトなモデルの重みづけは再度行うということです。
既に学習済みのモデルに入力を与え出力が得られたらその組をコンパクトなモデルに与えて学習させるイメージです

メリット
1. 精度が向上する可能性がある。教師モデルを超える可能性があります。
2. 教師モデルを複数用意し生徒モデルに適用することで膨大な知識を得る可能性があります。

# ベイズ最適化
ハイパーパラメータを含めた最適化問題とすることで、効率的なチューニングができる

# 多目的最適化
トレードオフの関係にある複数の目的関数を同時に求める最適化法です。 多目的最適化で得られる解は、一般にパレート解と呼ばれます。

# 人
## ジェフリー・ヒントン
ディープラーニングの父。今は、トロント大学の教授で、Google Brainプロジェクトの研究者でもある。
2006年にはオートエンコーダや深層信念ネットワークという手法を提唱。ディープラーニングの基礎を築く。
2012年には人工知能を用いて画像の認識力を競うコンテスト、「ILSVRC」ではディープラーニング（AlexNet）を用いて圧倒的に優勝する。

## アラン・チューリング
人工知能ができたかどうかを判定するテストである「チューリングテスト」を提唱。別の場所にいる人間がコンピュータと会話して、相手がコンピュータだと見抜けなければ知能があるとする。

## アンドリュー・ング
「Google Brain」や「coursera」の立ち上げにたずさわる。今はBaidu研究所に勤務。

## ヤン・ルカン
Facebookの人工知能研究所やニューヨーク大学に勤務。
LeNetと呼ばれる有名なCNNモデルを考えた。手書き数字を集めたデータセット「MNIST」を作った。
GANを高く評価した。

## レイ・カーツワイル
未来学者で「シンギュラリティ」という人工知能が人間よりも賢くなる年が来ることを予見する。

## ジョン・マッカーシー
パトリック・ヘイズとの共同論文でフレーム問題を提唱。余談だが、LISP言語を作った人。

## ジョセフ・アイゼンバウム
人工無能として有名な「ELIZA」を書き上げた人。

## ジョン・サール
強いＡＩ・弱いＡＩという用語を作った人。中国語の部屋という思考実験をする。

## ロジャー・ペンローズ
『皇帝の新しい心』という著書の中で「強いＡＩ」は実現できないと主張。

## ダニエル・デネット
フレーム問題の難しさを伝えるために、ロボットのたとえを挙げた。

## スティーブン・ホーキング
「人工知能の進化は人類の終焉を意味する」と発言。

## イーロン・マスク
人工知能を研究する非営利団体の一つである「Open AI」の創業者の一人。
人工知能に対して「人工知能にはかなり慎重に取り組む必要がある。結果的に悪魔を呼び出していることになるからだ。」という脅威論を述べる。

## オレン・エツィオーニ
人工知能に対して「コンピュータが世界制覇するという終末論的構想は『馬鹿げている』としか言いようがない」と発言し脅威論をけん制した。

## 福島邦彦
CNNの原型ともいえる、単純型細胞と複雑型細胞の２つの細胞の働きを組み込んだモデルである「ネオコグニトロン」を提唱。

## アーサー・サミュエル
機械学習を「明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野」と定義した。

## イアン・グッドフェロー
生成ネットワークと識別ネットワークからなる教師なし学習手法である敵対的生成ネットワークを提唱した。ヤン・ルカンはGANについて「機械学習において、この10年間で最も面白いアイデア」であると評価した。