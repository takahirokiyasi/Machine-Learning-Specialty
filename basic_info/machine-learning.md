# ベイスの定理
https://ai-trend.jp/basic-study/bayes/bayes-theorem/

# 勾配ブースティング
テーブルデータの教師あり学習において幅広いデータセットで高い精度を出すモデルとして知られている。弱い学習器を逐次的に学習するモデルである。
弱学習器としては具体的なモデルを仮定していない。
勾配降下法を使う

## 勾配ブースティング木（GBDT）
弱学習器に決定木を使用したもの
xgboost・lightGBMとかこれ

# ランダムフォレスト
バギングと呼ばれるサンプルの重複ありランダムサンプリングと各決定木で使用する特徴量のランダムサンプリングを行っている。

# ロジステック回帰
シグモイドが出力層なので0.5よりデカいか否か
## Class Probability
検出された値が正かどうかを判定する閾値

# サンプリング
[サンプリングについて](https://qiita.com/ryouta0506/items/619d9ac0d80f8c0aed92)
## オーバーサンプリング
少数派のデータをもとに不足分のデータを補完するというもの
特徴量間の相関が考慮できず、モデルの精度に影響する可能性があるという問題点がある。
それに対処するためにSMOTEがある
### SMOTE
大まかにいうと以下のステップで新しいデータを生成します。
1. タネとなるデータを抽出する。
1. タネとなるデータの近傍にあるデータを探索する。（探索対象数は事前設定要）
1. タネと探索したデータの間でランダムにデータを生成する。
こうすることで、特徴量間の相互関係を考慮したデータが生成できる

## アンダーサンプリング
少数派のデータ件数に合うように多数派データからランダムに抽出する方法

# 決定木における特徴量重要度
## 分割回数(frequency)
それぞれの特徴量の分割回数

# 正規化
最小値を0，最大値を1とする0-1スケーリング手法
異常に小さかったり、大きかったりする外れ値がある場合はデータが偏ってしまうので注意が必要
# 標準化
平均を0，分散を1とするスケーリング手法
最大値や最小値が決まっていないため特定のアルゴリズムで問題になったりする
一般的には標準化の方がよく使う

# 正則化
学習時に損失関数に正則化項を加え、これを目的関数として最適化を行います。これにより正則化項がパラメータの大きさに対するペナルティとして作用し、抑制がかかることで過学習を防ぐ効果が期待できる
[L1とL2正則化について](https://toeming.hatenablog.com/entry/2020/04/03/000925#:~:text=1.1.%20%E9%81%8E%E5%AD%A6%E7%BF%92%E3%81%A8%E6%AD%A3%E5%89%87,%E3%81%AE%E8%80%83%E3%81%88%E6%96%B9%E3%81%AF%E3%82%B7%E3%83%B3%E3%83%97%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82)
## L2正則化(Ridge)
円のやつ
L2正規化では正則化項をL2ノルムで定義。
L2ノルムとは各要素の二乗和の平方根をとったもので、"ユークリッド距離"、すなわち、いわゆる一般的な意味での"距離"

- 過学習を防ぐ
- 特徴量の削減(係数を0にする)ことには向いてない

## L1正則化(Lasso)
菱形のやつ
L1正規化では正則化項をL1ノルムで定義。
L1ノルムとは各要素の絶対値の和をとったもので、"マンハッタン距離"とも言われる。「碁盤の目状の町である2点間を移動するのに歩く距離」みたいなイメージ

最適解が軸上に位置するということは、最適解のその軸以外の要素が0であるということです。そしてモデルの係数が0になった場合、その係数がかかる特徴量はモデルの出力に影響を及ぼさないことになる￥。
つまり、L1正則化には`特徴量削減の効果がある`ということになる
スパース性に関係

- 不要な特徴量を削る(次元削減)
- 相関の高い特徴量をどちらも残すことができない

## ElasticNet
LassoとRidgeとのハイブリッドのようなもの

# スパース性
データの本質を表すような情報は、データ中に僅かしか含まれていないような状態

# バイアスとバリアンス
訓練をすればするほどバイアスは低くなるが、一方でバリアンスは高くなりがち（トレードオフの関係）
## バイアス
簡単にいうと実際値と予測値の差
## バリアンス
予測値の散らばってる度合い

# アンサンブル学習
## バギング
それぞれ学習した多数の弱学習器の多数決（分類の場合　回帰の場合は平均値）を取ることによって汎化性能を高めたアンサンブル学習のこと
並列的
## ブートストラップサンプリング（重複ありランダムサンプリング）
重複を許して無作為同数リサンプリングを反復する方法
ちなみに重複を許さないのがジャックナイフ法
## ブースティング
直列的に弱学習器を用いる。学習器を連続的に学習させて、より精度が向上するように修正していく
バイアスは下がりやすく、バリアンスが高くなりやすい、過学習に注意
## スタッキング
第一段階として学習器にランダムフォレストや勾配ブースティングなどのさまざまな計算法を使って複数のモデルを用意し学習し、予測値を出力（データセットは分けてあるモデルで学習に使ったデータセットと予測に使うデータセットはそれぞれ別にしないと過学習になる）
第一段階の予測値を取りまとめるモデルをメタモデルという。メタモデルは第一段階の予測値を特徴量として学習して、最終の予測値を出す。
メタモデルには回帰ならよく線形回帰モデルが使われるが特にアルゴリズムの決まりはない
[アンサンブル学習の種類](https://toukei-lab.com/ensemble#i-2)

# バッチ正規化
- 各層に伝わってきたデータを、その層でまた正規化するアプローチ。（最初に正規化をするだけでなく、層ごとに正規化を繰り返す）
- データの正規化、重みの初期化と比較し、より直接的な手法となる。
- 非常に強力な手法で学習がうまくいきやすく、オーバーフィッティングしにくい。
## 内部共変量シフト
入力時のxの分布が、学習途中で大きく変わってくることを指す。このシフトに合わせることにweightたちは躍起になるので、層自体の学習はそのあとでしか進まない。これのせいで遅くなる。
バッチ正規化は、このシフトを正規化してくれるもの。

# 活性化関数
活性化関数は、その数値を次のニューロンに「どのように出力するか」、つまり「どう活性化するか」が定義されたもの（※よって活性化関数で重要なのは、「数式自体」よりも「どのような形状のグラフになっているか」である）
ニューラルネットワークのニューロンにおける、入力のなんらかの合計（しばしば、線形な重み付け総和）から、出力を決定するための関数で、非線形な関数とすることが多い（※線形変換を何度重ねても線形にしか変化せず、意味がないため）

## 出力層の活性化関数 
`出力層の活性化関数`は次のニューロンに伝播するわけではないので、出力層の活性化関数には`恒等関数`と呼ばれる線形変換が用いられることもある。
出力層においてよく使われる主な活性化関数としては、
分類問題（二値）の場合は「シグモイド関数」
分類問題（多クラス）の場合は「ソフトマックス（Softmax）関数」
回帰問題の場合は「（活性化関数なし）」もしくは「恒等関数」

## シグモイド関数
微分したらまたシグモイドになるので無限に微分可能で嬉しい

# 勾配降下法
最小を求めるポイントは微分だが、勾配降下法だと計算が複雑になり、計算コストが高くなるので誤差逆伝播法が使われるようになった

## 具体的な手法
### バッチ勾配降下法
すべての学習データを使って勾配降下を行う。(ほとんど使わない)
### ミニバッチ勾配降下法
学習データから複数（バッチサイズ）を選択し誤差計算＆パラメータ更新を繰り返す。
### 確率的勾配降下法 (SGD)
学習データから確率的にデータを選択し、誤差計算＆パラメータ更新を行う。

## 勾配降下法の最適化アルゴリズム（オプティマイザー）
### モーメンタム
以前に適用した勾配の方向を、現在のパラメータ更新にも影響させる。（慣性を効かせる）
### AdaGrad
具体的には、勾配を二乗した値を蓄積し、すでに大きく更新されたパラメータほど更新量（学習率）を小さくする。
### AdaDelta
AdaGradの発展形
急速かつ単調な学習率の低下防止をはかったモデル。
### RMSprop
AdaGradの発展形
急速かつ単調な学習率の低下防止をはかったモデル。
Adadeltaと同時期に提唱されたもので、ほぼ同じの内容。
指数移動平均を蓄積することにより解決をはかったモデル。
### Adam
それぞれのパラメータに対し学習率を計算し適応させるモデル。
勾配の平均と分散をオンラインで推定した値を利用する。

# 誤差逆伝播法
勾配消失がデメリット

# 転移学習
学習済みモデルの中間層をそのまま利用して最終出力層のみ付け替えることで他の問題に適用する
ここで重要なのは`中間層の重みづけはそのまま利用`すること

メリット
1. 既に学習済であるため新規の学習データの量はデータは少なくてよい
2. 既に学習済であるため新規の学習データによる計算量は少ない

# ファインチューニング
学習済みモデルの最終出力層を付け替えるのですが、入力層に近い部分と出力層のパラメータも変更します。

# 蒸留
学習済みのモデルを使っており`コンパクト`なモデルを実現することです。転移学習との大きな違いはコンパクトなモデルの重みづけは再度行うということです。
既に学習済みのモデルに入力を与え出力が得られたらその組をコンパクトなモデルに与えて学習させるイメージです
過学習の緩和を目的として使用されることもある

メリット
1. 精度が向上する可能性がある。教師モデルを超える可能性があります。
2. 教師モデルを複数用意し生徒モデルに適用することで膨大な知識を得る可能性があります。

# ベイズ最適化
ハイパーパラメータを含めた最適化問題とすることで、効率的なチューニングができる

# 多目的最適化
トレードオフの関係にある複数の目的関数を同時に求める最適化法です。 多目的最適化で得られる解は、一般にパレート解と呼ばれます。

# RNN
前回の中間層の状態を隠れ層に入力する再帰構造を取り入れた、再帰型ニューラルネットワークで閉路がある
通時的誤差逆伝播法 BackPropagation Through-Time(BPTT) ： 時間軸に沿って誤差を反映していく。→勾配消失が問題
勾配消失が問題なのでLSTMに進化
## LSTM
ゲートが多いため計算量が多いからGRUに進化
機械翻訳や画像からのキャプション生成に応用できる
### LSTMブロック機構
CEC（Constant Error Carousel）：誤差を内部にとどまらせ`勾配消失を防ぐセル`。
ゲート：忘却ゲート、入力ゲート、出力ゲート構造をもつ

## GRU
LSTMの計算量を少なくした手法。
リセットゲート、更新ゲートからなる。

#　強化学習
期待値をとるのではなく、実際に行動を実施して次の時点の状態を確認しながら、少しずつQ値を更新していきます。実際に行動した結果のサンプルで期待値の代用としようという考え方で強化学習のアルゴリズムは作られている
行動者はエージェント
環境は状態を持っている

## Q学習
Q値を学習するためのアルゴリズムのひとつがQ学習
状態と行動の組に対してその後得られる報酬和の期待値（Q関数）を推定し、期待値が最大である行動を選択するアルゴリズム
## Sarsa
Q学習では、期待値の見積もりを現在推定されている値の最大値で置き換えましたが、Sarsa の場合、実際に行動してみたらどうなったかを使って期待値の見積もりを置き換えます。
ですので、Sarsa では現在の価値を更新するためには、エージェントが実際にもう一度行動をおこなう必要がある
## モンテカルロ法
モンテカルロ法はQ学習やSarsaとは違い、Q値の更新のときに「次の時点のQ値」を用いません。
代わりに、とにかく何らかの報酬が得られるまで行動をしてみて、その報酬値を知ってから、辿ってきた状態と行動に対してその報酬を分配して価値推定をする

## 方策勾配法
初期方策をパラメータを使って定義して、初期方策による行動を行い、得られた報酬から評価する
評価と方策から勾配方策定理という定理を使って方策勾配を求める（これにより評価が微小に大きくなるような方策の変更方法を求める）
方策を変更したら最初に戻りこれを繰り返す

ロボットアームを２７度傾けるなど連続の行動を扱いやすい

## モデルフリー・モデルベース
強化学習の手法は、大きく分けるとモデルフリー、モデルベースの二種類の分けられる。
さらにそれぞれ価値ベース、方策ベースの手法に分かれる

### モデルベースの手法
エージェントが環境の情報を完全に知っている問題に適用できる（エージェントが行動を起こさずとも全ての状態に関して、状態と行動に対応した報酬や状態遷移がわかっている問題）
３目並べなど
環境の情報が完全に手に入らない時は、行動による報酬の値を期待値として推定するために、価値ベースの手法と方策ベースの手法に分かれる

### 価値ベースの手法
行動を選択するために方策を固定する。

## Deep Q Network(DQN)
DQNでは、状態と価値をこれまで得た報酬で近似するQ関数を、ニューラルネットワークで表現する。
入力は状態で、出力層の各ノードは各行動の価値

# AlpahGo
教師あり学習フェーズと強化学習フェーズの２段階で囲碁の強さを高める
## 教師あり学習フェーズ
### Supeervised Learning Policy Network(SL Policy)
人間の譜面を教師データとして次の盤面を予測する
入力が現在の譜面状態で出力が次の譜面状態
### Rollout Policy
SL policyとほぼ同様
予測性能を下げる代わりに計算を高速にした

## 強化学習フェーズ
### Reinforcement Learning Policy Network(RL Policy)
SL Policy同士で対戦させ、勝った方を方策勾配法で強化をしていき、これを繰り返す
### Value Network
RL Policy同士で対戦し、各盤面の勝率を知る。
これを使って入力を現在の盤面状態　出力を入力盤面の勝率
教師データをRL Policyで作成した譜面で学習する

最後に学習済みの各ネットワークをモンテカルロ木探索（MCTS）のアルゴリズムを行動選択部分に組み込む

# 深層生成モデル
## VAE(変分オートエンコーダ)
画像を生成する潜在変数（入力データの特徴を圧縮）の分布を学習し、入力画像を平均と分散に変換する。
変換を行った後、デコーダの入力となる潜在変数をサンプリングすることで新たなデータを生成することができるようになる

## WaveNet
音声生成の分野に大きく影響を与えたモデル。最初の入力から次の出力を次々に予測していくというアプローチをベースにCNNで構成されている

# 自然言語処理
## 潜在的ディリクレ配分（Latent Dirichlet Allocation　LDA）
単語や文章を低次元のベクトルで表現できるという手法

- 類似文章の検索
- 文章や単語の分類
に使用される

### トピックモデルの推定
トピック＝文章集合に内在する話題
文章＝いくつかのトピックから出現した単語の集まり
- 各トピックからどんな単語が出やすいか
- 文章中に各トピックがどのぐらいの割合で混ざってるか
これらをベクトルで表現し、距離を計算する

## Tf-idf
レアな単語が何回も出てくるようなら、文書を分類する際にその単語の重要度を上げるというもの

文書に含まれる単語の重要度から文書の特徴を判別したり、文章間の類似度を計算できる