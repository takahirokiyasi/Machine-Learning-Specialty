# ベイスの定理
https://ai-trend.jp/basic-study/bayes/bayes-theorem/

# 勾配ブースティング
テーブルデータの教師あり学習において幅広いデータセットで高い精度を出すモデルとして知られている。弱い学習器を逐次的に学習するモデルである。
弱学習器としては具体的なモデルを仮定していない。
勾配降下法を使う

## 勾配ブースティング木（GBDT）
弱学習器に決定木を使用したもの
xgboost・lightGBMとかこれ

# ランダムフォレスト
バギングと呼ばれるサンプルの重複ありランダムサンプリングと各決定木で使用する特徴量のランダムサンプリングを行っている。

# ロジステック回帰
シグモイドが出力層なので0.5よりデカいか否か

# 決定木における特徴量重要度
## 分割回数(frequency)
それぞれの特徴量の分割回数

# 正規化
最小値を0，最大値を1とする0-1スケーリング手法
異常に小さかったり、大きかったりする外れ値がある場合はデータが偏ってしまうので注意が必要
# 標準化
平均を0，分散を1とするスケーリング手法
最大値や最小値が決まっていないため特定のアルゴリズムで問題になったりする
一般的には標準化の方がよく使う

# 正則化
学習時に損失関数に正則化項を加え、これを目的関数として最適化を行います。これにより正則化項がパラメータの大きさに対するペナルティとして作用し、抑制がかかることで過学習を防ぐ効果が期待できる
[L1とL2正則化について](https://toeming.hatenablog.com/entry/2020/04/03/000925#:~:text=1.1.%20%E9%81%8E%E5%AD%A6%E7%BF%92%E3%81%A8%E6%AD%A3%E5%89%87,%E3%81%AE%E8%80%83%E3%81%88%E6%96%B9%E3%81%AF%E3%82%B7%E3%83%B3%E3%83%97%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82)
## L2正則化(Ridge)
円のやつ
L2正規化では正則化項をL2ノルムで定義。
L2ノルムとは各要素の二乗和の平方根をとったもので、"ユークリッド距離"、すなわち、いわゆる一般的な意味での"距離"

- 過学習を防ぐ
- 特徴量の削減(係数を0にする)ことには向いてない

## L1正則化(Lasso)
菱形のやつ
L1正規化では正則化項をL1ノルムで定義。
L1ノルムとは各要素の絶対値の和をとったもので、"マンハッタン距離"とも言われる。「碁盤の目状の町である2点間を移動するのに歩く距離」みたいなイメージ

最適解が軸上に位置するということは、最適解のその軸以外の要素が0であるということです。そしてモデルの係数が0になった場合、その係数がかかる特徴量はモデルの出力に影響を及ぼさないことになる￥。
つまり、L1正則化には`特徴量削減の効果がある`ということになる
スパース性に関係

- 不要な特徴量を削る(次元削減)
- 相関の高い特徴量をどちらも残すことができない

## ElasticNet
LassoとRidgeとのハイブリッドのようなもの

# スパース性
データの本質を表すような情報は、データ中に僅かしか含まれていないような状態

# バイアスとバリアンス
訓練をすればするほどバイアスは低くなるが、一方でバリアンスは高くなりがち（トレードオフの関係）
## バイアス
簡単にいうと実際値と予測値の差
## バリアンス
予測値の散らばってる度合い

# アンサンブル学習
## バギング
それぞれ学習した多数の弱学習器の多数決（分類の場合　回帰の場合は平均値）を取ることによって汎化性能を高めたアンサンブル学習のこと
並列的
## ブートストラップサンプリング（重複ありランダムサンプリング）
重複を許して無作為同数リサンプリングを反復する方法
ちなみに重複を許さないのがジャックナイフ法
## ブースティング
直列的に弱学習器を用いる。学習器を連続的に学習させて、より精度が向上するように修正していく
バイアスは下がりやすく、バリアンスが高くなりやすい、過学習に注意
## スタッキング
第一段階として学習器にランダムフォレストや勾配ブースティングなどのさまざまな計算法を使って複数のモデルを用意し学習し、予測値を出力（データセットは分けてあるモデルで学習に使ったデータセットと予測に使うデータセットはそれぞれ別にしないと過学習になる）
第一段階の予測値を取りまとめるモデルをメタモデルという。メタモデルは第一段階の予測値を特徴量として学習して、最終の予測値を出す。
メタモデルには回帰ならよく線形回帰モデルが使われるが特にアルゴリズムの決まりはない
[アンサンブル学習の種類](https://toukei-lab.com/ensemble#i-2)

# バッチ正規化
- 各層に伝わってきたデータを、その層でまた正規化するアプローチ。（最初に正規化をするだけでなく、層ごとに正規化を繰り返す）
- データの正規化、重みの初期化と比較し、より直接的な手法となる。
- 非常に強力な手法で学習がうまくいきやすく、オーバーフィッティングしにくい。
## 内部共変量シフト
入力時のxの分布が、学習途中で大きく変わってくることを指す。このシフトに合わせることにweightたちは躍起になるので、層自体の学習はそのあとでしか進まない。これのせいで遅くなる。
バッチ正規化は、このシフトを正規化してくれるもの。

# 活性化関数
活性化関数は、その数値を次のニューロンに「どのように出力するか」、つまり「どう活性化するか」が定義されたもの（※よって活性化関数で重要なのは、「数式自体」よりも「どのような形状のグラフになっているか」である）
ニューラルネットワークのニューロンにおける、入力のなんらかの合計（しばしば、線形な重み付け総和）から、出力を決定するための関数で、非線形な関数とすることが多い（※線形変換を何度重ねても線形にしか変化せず、意味がないため）

## 出力層の活性化関数 
`出力層の活性化関数`は次のニューロンに伝播するわけではないので、出力層の活性化関数には`恒等関数`と呼ばれる線形変換が用いられることもある。
出力層においてよく使われる主な活性化関数としては、
分類問題（二値）の場合は「シグモイド関数」
分類問題（多クラス）の場合は「ソフトマックス（Softmax）関数」
回帰問題の場合は「（活性化関数なし）」もしくは「恒等関数」

## シグモイド関数
微分したらまたシグモイドになるので無限に微分可能で嬉しい

# 勾配降下法
最小を求めるポイントは微分だが、勾配降下法だと計算が複雑になり、計算コストが高くなるので誤差逆伝播法が使われるようになった

## 具体的な手法
### バッチ勾配降下法
すべての学習データを使って勾配降下を行う。(ほとんど使わない)
### ミニバッチ勾配降下法
学習データから複数（バッチサイズ）を選択し誤差計算＆パラメータ更新を繰り返す。
### 確率的勾配降下法 (SGD)
学習データから確率的にデータを選択し、誤差計算＆パラメータ更新を行う。

## 勾配降下法の最適化アルゴリズム（オプティマイザー）
### モーメンタム
以前に適用した勾配の方向を、現在のパラメータ更新にも影響させる。（慣性を効かせる）
### AdaGrad
具体的には、勾配を二乗した値を蓄積し、すでに大きく更新されたパラメータほど更新量（学習率）を小さくする。
### AdaDelta
AdaGradの発展形
急速かつ単調な学習率の低下防止をはかったモデル。
### RMSprop
AdaGradの発展形
急速かつ単調な学習率の低下防止をはかったモデル。
Adadeltaと同時期に提唱されたもので、ほぼ同じの内容。
指数移動平均を蓄積することにより解決をはかったモデル。
### Adam
それぞれのパラメータに対し学習率を計算し適応させるモデル。
勾配の平均と分散をオンラインで推定した値を利用する。

# 誤差逆伝播法
勾配消失がデメリット

# 転移学習
学習済みモデルの中間層をそのまま利用して最終出力層のみ付け替えることで他の問題に適用する
ここで重要なのは`中間層の重みづけはそのまま利用`すること

メリット
1. 既に学習済であるため新規の学習データの量はデータは少なくてよい
2. 既に学習済であるため新規の学習データによる計算量は少ない

# ファインチューニング
学習済みモデルの最終出力層を付け替えるのですが、入力層に近い部分と出力層のパラメータも変更します。

# 蒸留
学習済みのモデルを使っており`コンパクト`なモデルを実現することです。転移学習との大きな違いはコンパクトなモデルの重みづけは再度行うということです。
既に学習済みのモデルに入力を与え出力が得られたらその組をコンパクトなモデルに与えて学習させるイメージです

メリット
1. 精度が向上する可能性がある。教師モデルを超える可能性があります。
2. 教師モデルを複数用意し生徒モデルに適用することで膨大な知識を得る可能性があります。

# ベイズ最適化
ハイパーパラメータを含めた最適化問題とすることで、効率的なチューニングができる

# 多目的最適化
トレードオフの関係にある複数の目的関数を同時に求める最適化法です。 多目的最適化で得られる解は、一般にパレート解と呼ばれます。

# RNN
前回の中間層の状態を隠れ層に入力する再帰構造を取り入れた、再帰型ニューラルネットワークで閉路がある
通時的誤差逆伝播法 BackPropagation Through-Time(BPTT) ： 時間軸に沿って誤差を反映していく。→勾配消失が問題
勾配消失が問題なのでLSTMに進化
## LSTM
ゲートが多いため計算量が多いからGRUに進化
機械翻訳や画像からのキャプション生成に応用できる
### LSTMブロック機構
CEC（Constant Error Carousel）：誤差を内部にとどまらせ`勾配消失を防ぐセル`。
ゲート：忘却ゲート、入力ゲート、出力ゲート構造をもつ

## GRU
LSTMの計算量を少なくした手法。
リセットゲート、更新ゲートからなる。

#　強化学習
期待値をとるのではなく、実際に行動を実施して次の時点の状態を確認しながら、少しずつQ値を更新していきます。実際に行動した結果のサンプルで期待値の代用としようという考え方で強化学習のアルゴリズムは作られている
## Q学習
Q値を学習するためのアルゴリズムのひとつがQ学習
状態と行動の組に対してその後得られる報酬和の期待値（Q関数）を推定し、期待値が最大である行動を選択するアルゴリズム
## Sarsa
Q学習では、期待値の見積もりを現在推定されている値の最大値で置き換えましたが、Sarsa の場合、実際に行動してみたらどうなったかを使って期待値の見積もりを置き換えます。
ですので、Sarsa では現在の価値を更新するためには、エージェントが実際にもう一度行動をおこなう必要がある
## モンテカルロ法
モンテカルロ法はQ学習やSarsaとは違い、Q値の更新のときに「次の時点のQ値」を用いません。
代わりに、とにかく何らかの報酬が得られるまで行動をしてみて、その報酬値を知ってから、辿ってきた状態と行動に対してその報酬を分配して価値推定をする

## 方策勾配法
初期方策をパラメータを使って定義して、初期方策による行動を行い、得られた報酬から評価する
評価と方策から勾配方策定理という定理を使って方策勾配を求める（これにより評価が微小に大きくなるような方策の変更方法を求める）
方策を変更したら最初に戻りこれを繰り返す

ロボットアームを２７度傾けるなど連続の行動を扱いやすい

## モデルフリー・モデルベース
強化学習の手法は、大きく分けるとモデルフリー、モデルベースの二種類の分けられる。
さらにそれぞれ価値ベース、方策ベースの手法に分かれる

### モデルベースの手法
エージェントが環境の情報を完全に知っている問題に適用できる（エージェントが行動を起こさずとも全ての状態に関して、状態と行動に対応した報酬や状態遷移がわかっている問題）
３目並べなど
環境の情報が完全に手に入らない時は、行動による報酬の値を期待値として推定するために、価値ベースの手法と方策ベースの手法に分かれる

### 価値ベースの手法
行動を選択するために方策を固定する。

## Deep Q Network(DQN)
DQNでは、状態と価値をこれまで得た報酬で近似するQ関数を、ニューラルネットワークで表現する。
入力は状態で、出力層の各ノードは各行動の価値

# AlpahGo
教師あり学習フェーズと強化学習フェーズの２段階で囲碁の強さを高める
## 教師あり学習フェーズ
### Supeervised Learning Policy Network(SL Policy)
人間の譜面を教師データとして次の盤面を予測する
入力が現在の譜面状態で出力が次の譜面状態
### Rollout Policy
SL policyとほぼ同様
予測性能を下げる代わりに計算を高速にした

## 強化学習フェーズ
### Reinforcement Learning Policy Network(RL Policy)
SL Policy同士で対戦させ、勝った方を方策勾配法で強化をしていき、これを繰り返す
### Value Network
RL Policy同士で対戦し、各盤面の勝率を知る。
これを使って入力を現在の盤面状態　出力を入力盤面の勝率
教師データをRL Policyで作成した譜面で学習する

最後に学習済みの各ネットワークをモンテカルロ木探索（MCTS）のアルゴリズムを行動選択部分に組み込む

# 深層生成モデル
## WaveNet
音声生成の分野に大きく影響を与えたモデル。最初の入力から次の出力を次々に予測していくというアプローチをベースにCNNで構成されている

# 人
## ジェフリー・ヒントン
ディープラーニングの父。今は、トロント大学の教授で、Google Brainプロジェクトの研究者でもある。
2006年にはオートエンコーダや深層信念ネットワークという手法を提唱。ディープラーニングの基礎を築く。
2012年には人工知能を用いて画像の認識力を競うコンテスト、「ILSVRC」ではディープラーニング（AlexNet）を用いて圧倒的に優勝する。

## アラン・チューリング
人工知能ができたかどうかを判定するテストである「チューリングテスト」を提唱。別の場所にいる人間がコンピュータと会話して、相手がコンピュータだと見抜けなければ知能があるとする。

## アンドリュー・ング
「Google Brain」や「coursera」の立ち上げにたずさわる。今はBaidu研究所に勤務。

## ヤン・ルカン
Facebookの人工知能研究所やニューヨーク大学に勤務。
LeNetと呼ばれる有名なCNNモデルを考えた。手書き数字を集めたデータセット「MNIST」を作った。
GANを高く評価した。

## レイ・カーツワイル
未来学者で「シンギュラリティ」という人工知能が人間よりも賢くなる年が来ることを予見する。

## ジョン・マッカーシー
パトリック・ヘイズとの共同論文でフレーム問題を提唱。余談だが、LISP言語を作った人。

## ジョセフ・アイゼンバウム
人工無能として有名な「ELIZA」を書き上げた人。

## ジョン・サール
強いＡＩ・弱いＡＩという用語を作った人。中国語の部屋という思考実験をする。

## ロジャー・ペンローズ
『皇帝の新しい心』という著書の中で「強いＡＩ」は実現できないと主張。

## ダニエル・デネット
フレーム問題の難しさを伝えるために、ロボットのたとえを挙げた。

## スティーブン・ホーキング
「人工知能の進化は人類の終焉を意味する」と発言。

## イーロン・マスク
人工知能を研究する非営利団体の一つである「Open AI」の創業者の一人。
人工知能に対して「人工知能にはかなり慎重に取り組む必要がある。結果的に悪魔を呼び出していることになるからだ。」という脅威論を述べる。

## オレン・エツィオーニ
人工知能に対して「コンピュータが世界制覇するという終末論的構想は『馬鹿げている』としか言いようがない」と発言し脅威論をけん制した。

## 福島邦彦
CNNの原型ともいえる、単純型細胞と複雑型細胞の２つの細胞の働きを組み込んだモデルである「ネオコグニトロン」を提唱。

## アーサー・サミュエル
機械学習を「明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野」と定義した。

## イアン・グッドフェロー
生成ネットワークと識別ネットワークからなる教師なし学習手法である敵対的生成ネットワークを提唱した。ヤン・ルカンはGANについて「機械学習において、この10年間で最も面白いアイデア」であると評価した。

# 定理的なやつ
## トイ・プロブレム
第一次ブームの推論と探索では一見知的な活動を行えるようになったが、
あくまで適用範囲はルールとゴールが厳密に決まっている枠組みの中での話であり、
ルールが記述しきれず、またルールやゴールが曖昧である現実世界では全く役に立たないということ。

## フレーム問題
フレーム問題（フレームもんだい）とは、人工知能における重要な難問の一つで、有限の情報処理能力しかないロボットには、現実に起こりうる問題全てに対処することができないことを示すもの

## バーニーおじさんのルール(定理)
機械学習において学習に必要なデータ数は説明変数の数の10倍必要。

## モラベックのパラドックス
コンピュータに知能テストを受けさせたりチェッカーをプレイさせたりするよりも、1歳児レベルの知覚と運動のスキルを与える方が遥かに難しいか、あるいは不可能である

## ノーフリーランチ定理
万能な方法なんてないよってやつ
アーリーストッピングは万能だけど

## 次元の呪い
空間の次元が増えるのに対応して問題の算法が指数関数的に大きくなるということ

## シンボルグラウディング問題
自然言語に代表される記号Jに対応する表現がどのようなものであり、どのように獲得されるのかという問題

## トロッコ問題
倫理学上のジレンマ問題。

## 醜いアヒルの子定理
醜いアヒルの子とアヒルの子って鳥を全然知らん人からみたら、違いがわからんってやつ